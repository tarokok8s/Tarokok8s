#!/bin/bash

nn="dtm1.hdp"
rs="dtm2.hdp"


echo "[HDFS]"
nc -w 1 -z $nn 22 &>/dev/null
[ "$?" != 0 ] && echo '$nn exited' && exit 1
ssh -q $nn jps | grep 'NameNode' &>/dev/null
[ "$?" != 0 ] && echo "HDFS exited" && exit 1
ssh $nn hdfs dfsadmin -report &> /tmp/out 
cat /tmp/out | grep 'Live datanodes'
cat /tmp/out | grep 'Name: '
echo ""

echo "[YARN]"
nc -w 1 -z $rs 22 &>/dev/null
[ "$?" != 0 ] && echo '$rs exited' && exit 1
ssh $rs jps | grep 'ResourceManager' &>/dev/null
[ "$?" != 0 ] && echo "YARN exited" && exit 1
ssh $rs yarn node -list -all 2>/dev/null

echo""
echo [MapReduce]
ssh $nn hadoop jar /opt/dtp/hdp3.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar pi 2 1000 &> /tmp/out
cat /tmp/out | grep ' Pi '

#echo""
#ssh $rs hdfs dfs -ls /tmp/spark-events &>/dev/null
#[ "$?" != "0" ] && ssh $rs hdfs dfs -mkdir /tmp/spark-events && ssh $rs hdfs dfs -chmod -R 777 /tmp
#echo [Spark]
#ssh $rs spark-submit --num-executors 1 --executor-memory 512m \
#/opt/spark-3.4.0-bin-hadoop3/examples/src/main/python/pi.py 10 2>/dev/null

#echo ""
#echo [HBase]
#nc -w 1 -z dtm3.hdp 22 &>/dev/null
#[ "$?" != 0 ] && echo 'dtm3.hdp exited' && exit 1
#ssh -q dtm3.hdp jps | grep 'HMaster' &>/dev/null
#if [ "$?" == "0" ]; then
#   echo "status" | hbase shell -n 2>/dev/null | head -n 2
#fi
