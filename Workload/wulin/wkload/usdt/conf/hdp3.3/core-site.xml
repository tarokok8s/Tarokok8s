<?xml version="1.0" encoding="UTF-8"?>
<configuration>
 <property>
    <name>fs.defaultFS</name>
    <value>hdfs://dtm1.hdp:8020</value>
 </property>
 <property>
    <name>fs.default.name</name>
    <value>hdfs://dtm1.hdp:8020</value>
 </property>
 <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.GzipCodec</value>
 </property>

 <property>
    <name>hadoop.user.group.static.mapping.overrides</name>
    <value>hbase=bigboss;rbean=soup;gbean=soup,rice;ybean=rice</value>
 </property>

 <!--
   以下是設定 HttpFs 服務, HttpFs 必須用 root 啟動

   WebHDFS vs HttpFs Major difference between WebHDFS and HttpFs: WebHDFS needs access 
   to all nodes of the cluster and when some data is read it is transmitted from that node directly, 
   whereas in HttpFs, a singe node will act similar to a "gateway" and will be a single point of data 
   transfer to the client node. So, HttpFs could be choked during a large file transfer but the 
   good thing is that we are minimizing the footprint required to access HDFS
 -->
 <property>
    <name>hadoop.security.authorization</name>
    <value>true</value>
 </property>

 <!--
 <property>
    <name>hadoop.proxyuser.root.groups</name>
    <value>*</value>
 </property>

 <property>
    <name>hadoop.proxyuser.root.hosts</name>
    <value>*</value>
 </property>

   <property>
     <name>hadoop.proxyuser.super.hosts</name>
     <value>*</value>
   </property>
   <property>
     <name>hadoop.proxyuser.super.groups</name>
     <value>*</value>
   </property>
   -->

   <property>
     <name>hadoop.proxyuser.nfsserver.groups</name>
     <value>*</value>
   </property>
   <property>
      <name>hadoop.proxyuser.nfsserver.hosts</name>
      <value>*</value>
   </property>

   <property>
     <name>hadoop.proxyuser.bigred.groups</name>
     <value>*</value>
   </property>
   <property>
      <name>hadoop.proxyuser.bigred.hosts</name>
      <value>*</value>
   </property>

</configuration>
